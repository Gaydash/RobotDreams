{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f, types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/20 16:03:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/20 16:03:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/20 16:03:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/20 16:03:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('lect_13_home_task').getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "actor_df = spark.read.csv('./data/actor.csv', header=True, inferSchema=True)\n",
    "address_df = spark.read.csv('./data/address.csv', header=True, inferSchema=True)\n",
    "category_df = spark.read.csv('./data/category.csv', header=True, inferSchema=True)\n",
    "city_df = spark.read.csv('./data/city.csv', header=True, inferSchema=True)\n",
    "country_df = spark.read.csv('./data/country.csv', header=True, inferSchema=True)\n",
    "customer_df = spark.read.csv('./data/customer.csv', header=True, inferSchema=True)\n",
    "film_df = spark.read.csv('./data/film.csv', header=True, inferSchema=True)\n",
    "film_actor_df = spark.read.csv('./data/film_actor.csv', header=True, inferSchema=True)\n",
    "film_category_df = spark.read.csv('./data/film_category.csv', header=True, inferSchema=True)\n",
    "inventory_df = spark.read.csv('./data/inventory.csv', header=True, inferSchema=True)\n",
    "language_df = spark.read.csv('./data/language.csv', header=True, inferSchema=True)\n",
    "payment_df = spark.read.csv('./data/payment.csv', header=True, inferSchema=True)\n",
    "rental_df = spark.read.csv('./data/rental.csv', header=True, inferSchema=True)\n",
    "staff_df = spark.read.csv('./data/staff.csv', header=True, inferSchema=True)\n",
    "store_df = spark.read.csv('./data/store.csv', header=True, inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Домашнє завдання на тему Spark SQL\n",
    "\n",
    "Задачі з домашнього завдання на SQL потрібно розвʼязати за допомогою Spark SQL DataFrame API.\n",
    "\n",
    "- Дампи таблиць знаходяться в папці `data`. Датафрейми таблиць вже створені в клітинці вище.\n",
    "- Можете створювати стільки нових клітинок, скільки вам необхідно.\n",
    "- Розвʼязок кожної задачі має бути відображений в самому файлі (використати метод `.show()`)\n",
    "\n",
    "**Увага!**\n",
    "Використовувати мову запитів SQL безпосередньо забороняється, потрібно використовувати виключно DataFrame API!\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.\n",
    "Вивести кількість фільмів в кожній категорії.\n",
    "Результат відсортувати за спаданням."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|category_id|count_film|\n",
      "+-----------+----------+\n",
      "|         15|        74|\n",
      "|          9|        73|\n",
      "|          8|        69|\n",
      "|          6|        68|\n",
      "|          2|        66|\n",
      "|          1|        64|\n",
      "|         13|        63|\n",
      "|          7|        62|\n",
      "|         10|        61|\n",
      "|         14|        61|\n",
      "|          3|        60|\n",
      "|          5|        58|\n",
      "|         16|        57|\n",
      "|          4|        57|\n",
      "|         11|        56|\n",
      "|         12|        51|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# тут має бути розвʼязок задачі\n",
    "sql1 = film_category_df.groupBy('category_id').agg(f.count('film_id').alias('count_film')).sort('count_film')\n",
    "sql1.select('category_id', 'count_film').orderBy(f.desc('count_film')).show(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.\n",
    "Вивести 10 акторів, чиї фільми брали на прокат найбільше.\n",
    "Результат відсортувати за спаданням."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+\n",
      "|last_name|actor_name|amout_rental_films|\n",
      "+---------+----------+------------------+\n",
      "|   KILMER|    SANDRA|              2145|\n",
      "|    NOLTE|    WARREN|              2119|\n",
      "|   TEMPLE|     THORA|              1778|\n",
      "|DEGENERES|      NICK|              1611|\n",
      "|   KEITEL|     MILLA|              1586|\n",
      "|    BERRY|      KARL|              1480|\n",
      "|     TORN|    WALTER|              1478|\n",
      "|  HOFFMAN|     WOODY|              1455|\n",
      "|  GUINESS|      SEAN|              1425|\n",
      "|  GARLAND|     KEVIN|              1394|\n",
      "+---------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# тут має бути розвʼязок задачі\n",
    "join1 = rental_df.join(inventory_df, rental_df.inventory_id==inventory_df.inventory_id, 'inner').join(film_actor_df, inventory_df.film_id==film_actor_df.film_id, 'inner').join(actor_df, actor_df.actor_id==film_actor_df.actor_id, 'inner')\n",
    "\n",
    "result1 = join1.groupBy('last_name').agg(\n",
    "    f.max('first_name').alias('actor_name'),\n",
    "    f.count('rental_id').alias('amout_rental_films')\n",
    ").orderBy(f.desc(f.count('rental_id'))).show(10)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.\n",
    "Вивести категорія фільмів, на яку було витрачено найбільше грошей\n",
    "в прокаті"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     name|   amount_of_money|\n",
      "+---------+------------------+\n",
      "|   Sports| 5314.209999999848|\n",
      "|   Sci-Fi|  4756.97999999987|\n",
      "|Animation| 4656.299999999864|\n",
      "|    Drama| 4587.389999999876|\n",
      "|   Comedy| 4383.579999999895|\n",
      "|   Action| 4375.849999999871|\n",
      "|      New| 4361.569999999897|\n",
      "|    Games| 4281.329999999893|\n",
      "|  Foreign| 4270.669999999888|\n",
      "|   Family|4226.0699999998815|\n",
      "+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# тут має бути розвʼязок задачі\n",
    "join2 = payment_df.join(rental_df, rental_df.rental_id==payment_df.rental_id, 'inner').join(inventory_df, inventory_df.inventory_id==rental_df.inventory_id, 'inner').join(film_category_df, film_category_df.film_id==inventory_df.film_id, 'inner').join(category_df, category_df.category_id==film_category_df.category_id, 'inner')\n",
    "\n",
    "result1 = join2.groupBy('name').agg(\n",
    "    f.sum('amount').alias('amount_of_money')\n",
    ").orderBy(f.desc(f.sum('amount'))).show(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.\n",
    "Вивести назви фільмів, яких не має в inventory.\n",
    "Запит має бути без оператора IN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|APOCALYPSE FLAMINGOS|\n",
      "|    DAUGHTER MADIGAN|\n",
      "|       FANTASIA PARK|\n",
      "|       FRIDA SLIPPER|\n",
      "|   CURTAIN VIDEOTAPE|\n",
      "|         DANCES NONE|\n",
      "|    DARLING BREAKING|\n",
      "|DRIFTER COMMANDMENTS|\n",
      "|         FURY MURDER|\n",
      "|         KARATE MOON|\n",
      "|   MEMENTO ZOOLANDER|\n",
      "|        PURPLE MOVIE|\n",
      "|      REQUIEM TYCOON|\n",
      "|        CIDER DESIRE|\n",
      "|     CLUELESS BUCKET|\n",
      "|        DOGMA FAMILY|\n",
      "|         PURE RUNNER|\n",
      "|           RACER EGG|\n",
      "|      ROSES TREASURE|\n",
      "|         STOCK GLASS|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# тут має бути розвʼязок задачі\n",
    "join3 = film_df.join(inventory_df, inventory_df.film_id==film_df.film_id, 'left_outer')\n",
    "\n",
    "join3.select('title').distinct().show(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5.\n",
    "Вивести топ 3 актори, які найбільше зʼявлялись в категорії фільмів “Children”"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+\n",
      "|first_name|last_name|count(film_id)|\n",
      "+----------+---------+--------------+\n",
      "|     HELEN|   VOIGHT|             7|\n",
      "|     SUSAN|    DAVIS|             6|\n",
      "|     KEVIN|  GARLAND|             5|\n",
      "+----------+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# тут має бути розвʼязок задачі\n",
    "children_ct_df = category_df.select('category_id').where(category_df.name == 'Children')\n",
    "join5 = children_ct_df.join(film_category_df, film_category_df.category_id==children_ct_df.category_id, 'inner').join(film_actor_df, film_actor_df.film_id==film_category_df.film_id, 'inner').join(actor_df, actor_df.actor_id==film_actor_df.actor_id, 'inner')\n",
    "\n",
    "result1 = join5.groupBy(actor_df['first_name'], actor_df['last_name']).agg(\n",
    "    f.count(film_category_df['film_id'])\n",
    ").orderBy(f.desc(f.count(film_category_df['film_id']))).show(3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6.\n",
    "Вивести міста з кількістю активних та неактивних клієнтів\n",
    "(в активних customer.active = 1).\n",
    "Результат відсортувати за кількістю неактивних клієнтів за спаданням."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# тут має бути розвʼязок задачі\n",
    "join6 = customer_df.join(address_df, address_df.address_id==customer_df.address_id, 'inner').join(city_df, city_df.city_id==address_df.city_id, 'inner')\n",
    "\n",
    "#df3 = customer_df.withColumn(\"inactive_customers\", expr(\"case when active = 0 then 1 \" + \"else 0 end\"))\n",
    "\n",
    "#df4 = customer_df.select(col(\"*\"), expr(\"case when active == 0 then 1 \" + \"else 0 end\").alias(\"inactive_customers\"))\n",
    "#df2 = customer_df.withColumn(\"inactive_customers\", when(col(\"active\") == 0, 1).otherwise(0))\n",
    "#customer_df.select()\n",
    "\n",
    "def inact(x: int) -> int:\n",
    "    if x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "inactive_cust = customer_df.select('customer_id', 'address_id', 'active').withColumn('active', inact(f.col('active')))\n",
    "\n",
    "# join7 = customer_df.join(address_df, address_df.address_id==customer_df.address_id, 'inner').join(city_df, city_df.city_id==address_df.city_id, 'inner')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- address_id: integer (nullable = true)\n",
      " |-- activebool: boolean (nullable = true)\n",
      " |-- create_date: timestamp (nullable = true)\n",
      " |-- last_update: timestamp (nullable = true)\n",
      " |-- active: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [57], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m----> 7\u001B[0m customer_df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcustomer_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maddress_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mactive\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minactive\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[43minactive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mactive\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m20\u001B[39m)\n",
      "Cell \u001B[0;32mIn [57], line 2\u001B[0m, in \u001B[0;36minactive\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minactive\u001B[39m(x: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m :\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/RobotDreams/venv/pyspark/lib/python3.9/site-packages/pyspark/sql/column.py:1044\u001B[0m, in \u001B[0;36mColumn.__nonzero__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1043\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__nonzero__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1044\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1045\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot convert column into bool: please use \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m&\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mor\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1046\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m~\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m when building DataFrame boolean expressions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1047\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "\n",
    "def inactive(x: int) -> int:\n",
    "    if x==0 :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "customer_df.select('customer_id', 'address_id', 'active').withColumn('active', inactive(f.col('active'))).show(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
